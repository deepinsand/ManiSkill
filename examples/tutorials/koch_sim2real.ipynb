{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/haosulab/ManiSkill/blob/koch-v1.1/examples/tutorials/koch_sim2real.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koch v1.1 Sim2Real Tutorial (BETA)\n",
    "\n",
    "This notebooks covers a general sim2real pipeline:\n",
    "1. Setting up the sim environment\n",
    "2. Setting up the real environment\n",
    "2. Aligning cameras between sim and real\n",
    "4. Training your agent with PPO\n",
    "5. Zero-Shot Evaluation on real robot\n",
    "\n",
    "Following the steps of this pipeline, you will replicate:\n",
    "\n",
    "MS3 custom sim training\n",
    "\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_grab_training.gif?raw=true) \n",
    "\n",
    "Zero-Shot Eval on real\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_grabcube.gif?raw=true)\n",
    "\n",
    "**Required Materials:**\n",
    "\n",
    "The comprehensive list of physical items necessary for the real environment:\n",
    "* local machine (local gpu not required)\n",
    "* Koch v1.1 follower arm\n",
    "* phone (or any device) camera \n",
    "* camera tripod TODO (xhin): show 3d printed tripod here\n",
    "* 3D printed 2cm sidelength cube TODO (xhin): list stl here\n",
    "* measuring tape / ruler (for camera alignment)\n",
    "\n",
    "*We also use a single lightsource of white light in our real evaluation environment\n",
    "\n",
    "**NOTE:**\n",
    "* All GPU operations in this tutorial can run on Colab's free tier, but a local machine is still required for controlling the actual robot.\n",
    "* This project currently is in a **beta release**, so not all features have been added in yet and there may be some bugs. If you find any bugs or have any feature requests please post them to our [GitHub issues](https://github.com/haosulab/ManiSkill/issues/) or discuss about them on [GitHub discussions](https://github.com/haosulab/ManiSkill/discussions/). We also have a [Discord Server](https://discord.gg/x8yUZe5AdN) through which we make announcements and discuss about ManiSkill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, prepare the colab environment by switching to a GPU environment (Runtime -> Change Runtime Type)\n",
    "\n",
    "Then install/import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup vulkan\n",
    "!mkdir -p /usr/share/vulkan/icd.d\n",
    "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/nvidia_icd.json\n",
    "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/10_nvidia.json\n",
    "!mv nvidia_icd.json /usr/share/vulkan/icd.d\n",
    "!mv 10_nvidia.json /usr/share/glvnd/egl_vendor.d/10_nvidia.json\n",
    "!apt-get install -y --no-install-recommends libvulkan-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    import site\n",
    "    !pip install git+https://github.com/haosulab/ManiSkill.git@koch-v1.1\n",
    "    site.main() # run this so local pip installs are recognized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Digital Twins Simulation Evironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Understanding the Sim Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to customizing our environments, lets first \n",
    "1. Create the default environment - used for all of the training and evaluations on our end\n",
    "2. Understand robot input & output\n",
    "3. Understand the sim robot controller\n",
    "4. Control our sim robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Making the Default Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our imports\n",
    "import mani_skill.envs\n",
    "from mani_skill.utils.wrappers.flatten import FlattenRGBDObservationWrapper\n",
    "\n",
    "# basic imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# Note: Warning aobut Greenscreening ok and will be covered in a later section\n",
    "example_env_kwargs = dict(obs_mode=\"rgb+segmentation\", render_mode=\"rgb_array\", control_mode=\"pd_joint_delta_pos\", num_envs=1, sim_backend=\"cpu\")\n",
    "sim_env = gym.make(\"GrabCube-v1\", dr=False, **example_env_kwargs)\n",
    "sim_obs, _ = sim_env.reset()\n",
    "\n",
    "print(\"simulation render:\")\n",
    "plt.imshow(sim_env.render()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Understanding Policy Input & Output\n",
    "\n",
    "Observations are either state or camera observations\n",
    "\n",
    "All state observations we use are available / estimatable during runtime in real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy input example - observing the observations\n",
    "\n",
    "# state observations\n",
    "state_obs = sim_obs[\"extra\"]\n",
    "# camera observations\n",
    "camera_obs = sim_obs[\"sensor_data\"][\"base_camera\"][\"rgb\"][0].cpu()\n",
    "\n",
    "print(\"state observations:\")\n",
    "for obs in state_obs:\n",
    "    print(obs, state_obs[obs])\n",
    "print() \n",
    "print(\"camera observations:\")\n",
    "plt.imshow(camera_obs); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A vigilant reader may notice that we use the state based boolean value of is_grasped in our observations__\n",
    "* While this is priveledged state based info in sim, all state observations in real are functions of qpos and target_qpos\n",
    "* In real, we estimate is_grasped by looking for a large difference in the target and actual qpos of the gripper joint. See [Section 6.1](#6-appendix) for the code implementation\n",
    "\n",
    "Now, lets take a look at policy output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy output example - environment action space\n",
    "sample_action = sim_env.action_space.sample()\n",
    "print(\"sample_action\", sample_action)\n",
    "print(\"sample_action shape\", sample_action.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Policy Raw Output to Robot Control Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use MS3's pd_joint_target_delta_pos controller (best performance on real robot empirically), meaning:\n",
    "* Policy (neural network) output logits are clipped between [-1,1] then scaled between ```lower``` and ```upper``` controller parameters\n",
    "* The clipped and scaled output is used for target delta control, where:\n",
    "    * qpos (length 6) are the current joint positions of the robot at each step\n",
    "    * target_qpos (length 6) are the joint positons the robot is attempting to match at each step\n",
    "    * these clipped and scaled outputs (also length 6) are added directly to target_qpos at each step\n",
    "\n",
    "The config for the controller can be found in ```ManiSkill/mani_skill/agents/robots/koch/koch.py``` and looks like:\n",
    "```python\n",
    "pd_joint_target_delta_pos = PDJointPosControllerConfig(\n",
    "    [joint.name for joint in self.robot.active_joints],\n",
    "    lower=[-0.05, -0.05, -0.05, -0.05, -0.1, -0.05],\n",
    "    upper=[0.05, 0.05, 0.05, 0.05, 0.1, 0.05],\n",
    "    stiffness=[123, 50, 102.68, 145, 108.37, 93.3],\n",
    "    damping=[15.85, 6, 15.34, 16, 16.31, 16.3],\n",
    "    force_limit=100,\n",
    "    use_delta=True,\n",
    "    use_target=True,\n",
    ")\n",
    "```\n",
    "\n",
    "We will reuse this exact controller for the real robot evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Controlling The Sim Robot\n",
    "\n",
    "rerun the following cell as many times as you'd like! (it may take many steps for noticeable difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#sim_env.reset() # uncomment this to reset the env, make it a comment to stop resetting\n",
    "\n",
    "random_action = sim_env.action_space.sample()\n",
    "close_gripper_only = np.array([0,0,0,0,0,-1.0])\n",
    "action = close_gripper_only # try random_action also!\n",
    "sim_obs, _, _, _, _ = sim_env.step(action)\n",
    "\n",
    "plt.imshow(sim_obs[\"sensor_data\"][\"base_camera\"][\"rgb\"][0].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Choosing Simulation Camera Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make your own digital twin... a twin, this pipeline estimates sim camera pose, by:\n",
    "\n",
    "* Setting necessary LookAt transform parameters in sim: \n",
    "    * camera position\n",
    "    * camera target position\n",
    "    * camera fov\n",
    "* Choosing 3D alignment point(s) for camera alignment:\n",
    "    * set the alignment dot(s) so they span your simulation camera's field of view (i.e. they are not all in one place)\n",
    "    * measuring them out carefully in real ([section 2](#2-setting-up-your-real-env))\n",
    "* Aligning real camera pose(s) with sim, using our script ([section 3](#3-matching-real-camera-pose-with-sim-camera))\n",
    "\n",
    "The following code cell sets the **_LookAt transform and camera alignment parameters_** for the simulated environment\n",
    "\n",
    "* **We recommend leaving the default values for now, coming back to change them as necessary once you set up your real environment**\n",
    "\n",
    "* **Differing camera positions are currently untested and may make perception more difficult**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Simulation: Coordinates are in meters, and they are centered at the robot's base (at the edge of the table, pictured below):\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_base_pos.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User parameters for LookAt transform####\n",
    "# NOTE: all values are offsets from robot base position (see following cell)\n",
    "robot_base_color = [0.95,0.95,0.95]\n",
    "\n",
    "base_camera_pos = [0.40, 0 + 0.265, 0.1725]\n",
    "camera_target = [0.2, 0, 0] # green dot in sim debug mode\n",
    "\n",
    "# tune this according to your camera\n",
    "# TODO (xhin): should honestly be a whole different section / reference a tutorial for camera intrinsics estimation\n",
    "camera_fov = 52* (np.pi / 180)\n",
    "\n",
    "# debug camera position points - appear if debug in rgb_overlay_mode, useful in camera alignment\n",
    "# really only need 2, but more is helpful\n",
    "alignment_dots = [ # yellow dots in sim debug mode\n",
    "    [0.2,  0.10, 0], ## close to camera\n",
    "    [0.2,  -0.1, 0], ## far from camera\n",
    "    [0.35, 0.00, 0], ## far infront of robot\n",
    "    [0.35, 0.10, 0], ## far infront of robot and close camera\n",
    "]\n",
    "#### End User parameters ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render with Camera Parameters\n",
    "dubug = True # toggles visibility of alignment dots\n",
    "# all user params from cell above applied via env kwargs\n",
    "example_env_kwargs.update(robot_base_color=robot_base_color,\n",
    "                          base_camera_pos=base_camera_pos,\n",
    "                          camera_target=camera_target,\n",
    "                          camera_fov=camera_fov,\n",
    "                          alignment_dots=alignment_dots)\n",
    "sim_env = gym.make(\"GrabCube-v1\", dr=False, debug=dubug, **example_env_kwargs)\n",
    "sim_obs, _ = sim_env.reset(seed=1)\n",
    "\n",
    "\n",
    "sim_env.unwrapped.toggle_greenscreen(False) # covered in section 1.3\n",
    "env_render = sim_env.render().cpu()\n",
    "print()\n",
    "print(\"Visualization of camera alignment dots: Alignment dots appear if env 'debug' mode is on\")\n",
    "print(\"The Green dot marks camera lookat target (center of rendered image)\")\n",
    "print(\"Yellow dots mark camera alignment dots\")\n",
    "plt.imshow(env_render[0]); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 Saving Custom Env Params to Disk\n",
    "* The cell below stores the custom parameters you just set in a json file to use later for camera alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_env.unwrapped.save_user_kwargs(\"user_kwargs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Domain Randomizations:\n",
    "\n",
    "A deeper-dive into domain randomizations and their valuse can be found in [section 6.2](#62-implemented-domain-randomization)\n",
    "\n",
    "However, a comprehensive list of randomizations already implemented is:\n",
    "- robot color\n",
    "- robot pose\n",
    "- camera position\n",
    "- camera lookat position\n",
    "- camera rotation about the viewing direction\n",
    "- subscene lighting\n",
    "- cube color\n",
    "- cube friction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 GreenScreening in ManiSkill3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can overlay our robot and scene actors on any image we like, for example: a green-screen\n",
    "* MS3 stores segmentation data available during rasterization, at each rendering step\n",
    "* Later, we use the real environment background (from step 3) for simulation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a \"green-screen\"\n",
    "dummy_greenscreen = np.zeros((128,128,3))\n",
    "dummy_greenscreen[:,:,1] = 0.75\n",
    "plt.imsave(\"dummy_greenscreen.png\", dummy_greenscreen)\n",
    "\n",
    "# temporarily turn off domain randomiations\n",
    "domain_rands = False\n",
    "\n",
    "# now we can create the sim environment, and view it\n",
    "# obs_mode=\"rgb+segmentation\" required for greenscreening \n",
    "# reusing user params saved within example_env_kwargs \n",
    "sim_env = gym.make(\"GrabCube-v1\", rgb_overlay_path=\"dummy_greenscreen.png\", dr=domain_rands, **example_env_kwargs)\n",
    "sim_obs, _ = sim_env.reset(seed=0)\n",
    "\n",
    "# view the simulation renders\n",
    "\n",
    "# toggle off greenscreening for render - note that greenscreening for sim_env.get_obs() stays on!\n",
    "# greenscreening activated by default\n",
    "sim_env.unwrapped.toggle_greenscreen()\n",
    "print(\"Green-Screening off\")\n",
    "plt.imshow(sim_env.render().cpu()[0]); plt.show()\n",
    "\n",
    "# toggle back on greenscreeening\n",
    "sim_env.unwrapped.toggle_greenscreen()\n",
    "print(\"Green-Screening On (render resolution adjusted to env observation res 128x128)\")\n",
    "plt.imshow(sim_env.render().cpu()[0]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the greenscreen can be anything we'd like\n",
    "!wget -O other_greenscreen.png https://www.dinehq.com/uploads/maniskill.png > /dev/null 2>&1\n",
    "sim_env.unwrapped.set_overlay(camera_name=\"base_camera\", path=\"other_greenscreen.png\")\n",
    "print(\"Green-Screen Changed (render resolution adjusted to env observation res 128x128)\")\n",
    "plt.imshow(sim_env.render().cpu()[0]); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Setting Up your Real Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Setting up your Koch v1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.1 Before We Can Start\n",
    "\n",
    "1. You first need to follow [Lerobot's tutorial for the Koch v1.1 robot setup](https://github.com/huggingface/lerobot/blob/380b836eee5f7cd61f023ef09dd5e0d9721d8c54/examples/7_get_started_with_real_robot.md). It will result in the creation of a calibration directory under ```.cache/calibration/koch``` required for this tutorial.\n",
    "\n",
    "2. Once finished, git clone MS3 on local: ```git clone https://github.com/haosulab/ManiSkill.git```\n",
    "3. Finally, move the calibration directory under ```.cache/calibration/koch``` into the parent of the local, cloned root directory: ```ManiSkill/..```\n",
    "4. 3D print cube(s) using our stl: TODO xhin: add stl and path here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.2 Compatibility for MS3\n",
    "\n",
    "Next, find our koch real robot wrapper in ```mani_skill/envs/tasks/digital_twins/real_agents/koch.py```\n",
    "\n",
    "Update the ports, cameras, and calibration directory as needed:\n",
    "\n",
    "```python\n",
    "class MS3RealKoch(BaseRealAgent):\n",
    "    ...\n",
    "    def _load_agent(self, **kwargs):\n",
    "        \"\"\"Conect the robot\"\"\"\n",
    "        robot_config = KochRobotConfig(\n",
    "            leader_arms={},\n",
    "            follower_arms={\n",
    "                \"main\": DynamixelMotorsBusConfig(\n",
    "                    port=\"/dev/ttyACM0\", # <-- UPDATE HERE\n",
    "                    motors={\n",
    "                        # name: (index, model)\n",
    "                        \"shoulder_pan\": [1, \"xl430-w250\"],\n",
    "                        \"shoulder_lift\": [2, \"xl430-w250\"],\n",
    "                        \"elbow_flex\": [3, \"xl330-m288\"],\n",
    "                        \"wrist_flex\": [4, \"xl330-m288\"],\n",
    "                        \"wrist_roll\": [5, \"xl330-m288\"],\n",
    "                        \"gripper\": [6, \"xl330-m288\"],\n",
    "                    },\n",
    "                ),\n",
    "            },\n",
    "            cameras={\n",
    "                \"base_camera\": OpenCVCameraConfig(\n",
    "                    camera_index=2,  # <-- UPDATE HERE\n",
    "                    fps=60,\n",
    "                    width=640,\n",
    "                    height=480,\n",
    "                    rotation=90,  # <--- <-- UPDATE HERE If Necessary\n",
    "                ),\n",
    "            },\n",
    "            calibration_dir=\"koch_calibration\",  # <-- UPDATE HERE\n",
    "        )\n",
    "        robot = ManipulatorRobot(robot_config)\n",
    "        robot.connect()\n",
    "        return robot\n",
    "```\n",
    "\n",
    "Note that the leader arms is left empty - you will not need the leader arm for this tutorial\n",
    "\n",
    "The MS3RealKoch wrapper makes the real koch-v1.1 robot compatible with ManiSkill3 trained policies, mainly it:\n",
    "- Handles formatting of RGB and proprioceptive observations for policy input\n",
    "- Allows use of ManiSKill3's controllers on the real robot\n",
    "\n",
    "If you're curious, feel free to look deeper into the wrapper at  ```mani_skill/envs/tasks/digital_twins/real_agents/koch.py```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Drawing the Alignment Dots\n",
    "* Draw the alignment dots from sim in [section 1.2](#12-choosing-simulation-camera-parameters) on your real table (more accurate) or place your choice of removable markers on the 3D alignment points we passed into the environment earlier\n",
    "    * Color/shape/size doesn't matter here, this is only for aligning the camera\n",
    "    * measure them out with their exact coordinates from 1.2\n",
    "        - ex: [0.35, 0.10, 0.0] is 35 cm in front and 10 cm to the right of the robot (closest dot to camera in pictured below)\n",
    "* Similarly, draw/place a dot for the camera target position\n",
    "* Finally, measure and place your camera at chosen 3D point, and roughly align it to scene (camera alignment in [section 3](#3-matching-real-camera-pose-with-sim-camera))\n",
    "\n",
    "After this step, our setup looks like this:\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_real_init.png?raw=true)\n",
    "\n",
    "**Note:** you do not need to perfectly pinpoint this robot base position\n",
    "* An estimate point on the edge of the table will suffice; the robot can be further adjusted during camera alignment in [section 3](#3-matching-real-camera-pose-with-sim-camera). \n",
    "\n",
    "**Reminder** Coordinates are in meters, and they are centered at the robot's base (at the edge of the table, pictured below):\n",
    "* Measure out the alignment dots using the values set (default or your chosen values) in [section 1.2](#12-choosing-simulation-camera-parameters), centered at point visualized below\n",
    "* If the default values are incompatible with your setup requirements, this is where you would change the camera setup parameters in [section 1.2](#12-choosing-simulation-camera-parameters)\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_base_pos.png?raw=true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Matching Real Camera Pose with Sim Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script overlays the simulation actors onto your real world setup\n",
    "\n",
    "1 Download and paste ```user_kwargs.json``` from this colab into the local ```ManiSkill/..``` directory, for custom env replication on local\n",
    "\n",
    "2 On local and within the parent of the cloned root directory: ```ManiSkill/..```\n",
    "* plug in your camera & follower arm\n",
    "* Run: ```python3 ManiSkill/mani_skill/envs/tasks/digital_twins/utils/camera_setup.py --user_kwargs_path=user_kwargs.json```\n",
    "    * You will be prompted to press 'Enter' before the robot moves to the initial pose of the environment\n",
    "\n",
    "\n",
    "Our Aligment process looks like:\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_cam_align.gif?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Taking the Greenscreen Photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have aligned your camera, you can take the background photo for greenscreened simulation training:\n",
    "* Move your koch_v1.1 follower arm out of camara view\n",
    "* Run ```python3 ManiSkill/mani_skill/envs/tasks/digital_twins/utils/camera_setup.py --output_photo_path=<your_path_here>.png```\n",
    "\n",
    "This will output a photo at ```<your_path_here>_base_camera.png```\n",
    "\n",
    "If you're in Colab:\n",
    "* Copy and paste this photo into the colab files for simulation training in next step\n",
    "\n",
    "Feel free to re-run camera alignment from previous step to move koch_v1.1 back into its proper place, without moving the camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final background image looks like (yours may be 128x128 instead): \n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_example_greenscreen.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, visualize your final greenscreen in sim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_greenscreen_path = ... # put your path here\n",
    "example_env_kwargs.update(rgb_overlay_path=colab_greenscreen_path)\n",
    "sim_env = gym.make(\"GrabCube-v1\", dr=False, **example_env_kwargs)\n",
    "sim_obs, _ = sim_env.reset(seed=0)\n",
    "plt.imshow(sim_obs[\"sensor_data\"][\"base_camera\"][\"rgb\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-save the user_kwargs for the environment, this saves your greenscreen path as well - to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_env.unwrapped.save_user_kwargs(\"user_kwargs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Training your Agent in Sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MS3 provdes an array of rl training scripts within ```ManiSkill/examples/baselines```\n",
    "\n",
    "Below, we provide an example using the modules from MS3's ```ppo_rgb.py``` to train an agent with the environment you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/haosulab/ManiSkill/koch-v1.1/examples/baselines/ppo/ppo_rgb.py -O ppo_rgb.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started first download the RL code as done below and import some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below opens tensorboard on colab so you can watch training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the script, change the colab to a GPU environment (Runtime -> Change Runtime Type)\n",
    "\n",
    "Evaluation rollout videos and model checkpoints will populate in the runs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ppo_rgb.py --env_id=\"GrabCube-v1\" --num_envs=256 \\\n",
    "    --update_epochs=4 --num_minibatches=8 --total_timesteps=15_000_000 \\\n",
    "    --num-steps=75 --num_eval_steps=75 --gamma=0.90 \\\n",
    "    --no_partial_reset --render_mode=rgb_array --reconfiguration_freq=1 \\\n",
    "    --obs_mode=rgb+segmentation --seed=0 --no_finite_horizon_gae \\\n",
    "    --exp_name=grab_cube_training --user_kwargs_path=user_kwargs.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Policy Evaluation on Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Safe Evaluation\n",
    "\n",
    "**NOTE**: Before running any scripts with the real robot, we suggest you ensure that your ports are correctly listed in the ```mani_skill/envs/tasks/digital_twins/real_agents/koch.py``` file. Use the ```find_motors_bus_port.py``` script from lerobot and re-permit use of the ports on local with chmod if necessary, then plug in your robots\n",
    "\n",
    "You can evaluate any checkpoint from training directly on the real robot:\n",
    "* Copy and paste the checkpoint, e.g. ckpt_776.pt, into your local directory of choice <your_dir>\n",
    "* Run the following script on local - the debug flag requires you to press enter at every environment step\n",
    "```\n",
    "python3 ManiSkill/mani_skill/envs/tasks/digital_twins/utils/real_ppo_agent_eval.py --real_env_id=\"RealGrabCube-v1\" --control_mode=\"pd_joint_delta_pos\" --num_eval_steps=200 --checkpoint=<your_dir>/ckpt_776.pt --debug\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.2 Normal Evaluation\n",
    "\n",
    "**Warning**: Be ready to unplug / control-c / stop the script at any time; it is the user's responsibility to ensure the robot does not hurt its own joints via colliding with the table, etc. There is no guarentee the policy will properly transfer to real. We recommend thourough evaulation with --debug flag before this step.\n",
    "\n",
    "The simulation training and recommended evaluation frequence is 15hz. To evaluate at 15hz, run:\n",
    "\n",
    "```python3 ManiSkill/mani_skill/envs/tasks/digital_twins/utils/real_ppo_agent_eval.py --real_env_id=\"RealGrabCube-v1\" --control_mode=\"pd_joint_delta_pos\" --num_eval_steps=200 --checkpoint=<your_dir>/ckpt_776.pt --control_freq=15```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.3 Our Evaluations:\n",
    "\n",
    "Our evaluations using this tutorial can be found in [ManiSkill3's Demo Gallery](https://maniskill.readthedocs.io/en/latest/user_guide/demos/gallery.html#vision-based-zero-shot-sim2real-manipulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Appendix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 The Real Env - 'is grasped' observation\n",
    "\n",
    "Below, we show the proprioceptive observations used in our real environment, within\n",
    " ```ManiSkill/mani_skill/envs/tasks/digital_twins/affordable_arms_tabletop/real_tasks/real_grab_cube.py```:\n",
    "\n",
    "```python\n",
    "@register_env(\"RealGrabCube-v1\", max_episode_steps=75)\n",
    "class RealGrabCubeEnv(BaseRealEnv):\n",
    "    real_agent_cls = MS3RealKoch\n",
    "    ...\n",
    "    # NOTE: In emulating maniskill environments, batching obs with batchsize=1 is required\n",
    "    # agent qpos is already included in state observations, from BaseRealEnv\n",
    "    def _get_obs_extra(self):\n",
    "        qpos = common.batch(self.agent.qpos.clone())\n",
    "        target_qpos = self.agent.controller._target_qpos.clone()\n",
    "        is_grasped = ((qpos[..., -1] - target_qpos[..., -1]) >= 0.02).float() * (\n",
    "            target_qpos[..., -1] < 0.24\n",
    "        )\n",
    "        obs.update(\n",
    "            qpos=qpos,\n",
    "            target_qpos=target_qpos,  # already a batched torch tensor: (1, #joints)\n",
    "            rest_qpos=common.batch(\n",
    "                self.robot_keyframe_qpos[:-1].clone()\n",
    "            ),  # already a torch tensor, need to batch: (#joints) -> (1, #joints)\n",
    "            to_rest_dist=common.batch(\n",
    "                self.robot_keyframe_qpos[:-1].clone() - qpos[0, :-1]\n",
    "            ),\n",
    "            is_grasped=is_grasped.view(1, 1),\n",
    "        )\n",
    "        return obs\n",
    "```\n",
    "\n",
    "The ```_get_obs_extra``` function includes all of the proprioceptive information fed to our policy at runtime\n",
    "\n",
    "- is grasped is an estimate based on a delta between the current and target joint positions\n",
    "- RGB observations are gathered and formattted by the MS3RealKoch wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Implemented Domain Randomization:\n",
    "\n",
    "Below, we show the default domain randomizations and their sampling values. Check out the full file for the full implemntation!\n",
    "\n",
    "We split domain randomizations into task-agnostic and task specific for the development of future tasks\n",
    "\n",
    "For the Koch arm (and future affordable arms), we use the Sim4RealBaseEnv in ```ManiSkill/mani_skill/envs/tasks/digital_twins/affordable_arms_tabletop/sim_tasks/sim4real_base_env.py``` for task-agnostic randomizations\n",
    "\n",
    "```python\n",
    "class Sim4RealBaseEnv(BaseDigitalTwinEnv):\n",
    "    \"\"\"\n",
    "    Base environment for simulating affordable robot tasks\n",
    "    General, non-task-specific randomizations to aid sim2real transfer\n",
    "    Randomizations include:\n",
    "        - robot color\n",
    "        - robot pose\n",
    "        - camera position\n",
    "        - camera lookat position\n",
    "        - camera rotation about the viewing direction\n",
    "        - subscene lighting\n",
    "    \"\"\"\n",
    "    def __init__( ... )\n",
    "        ...\n",
    "        ################ Task-Agnostic Randmoizations ################\n",
    "        # robot color noise\n",
    "        self.robot_color_noise = 0.05\n",
    "\n",
    "        # robot pose randomizations\n",
    "        self.robot_zrot_noise = (3 * np.pi / 180) * toggle_rand\n",
    "        self.robot_y_noise = 0.01 * toggle_rand  # cm offset\n",
    "        # camera randomizations\n",
    "        self.max_camera_offset = [\n",
    "            0.025 * toggle_rand,\n",
    "            0.025 * toggle_rand,\n",
    "            0.025 * toggle_rand,\n",
    "        ]\n",
    "        self.camera_target_noise = 1e-6\n",
    "        self.camera_view_rot_noise = 5e-3 * toggle_rand\n",
    "        self.camera_fov_noise_range = (\n",
    "            2.0 * (np.pi / 180) * toggle_rand\n",
    "        )  # max rad offset from camera_fov to sample from\n",
    "        ################ End Task-Agnostic Randmoizations #############\n",
    "        ...\n",
    "```\n",
    "\n",
    "The grab cube implementation can be found in ```ManiSkill/mani_skill/envs/tasks/digital_twins/affordable_arms_tabletop/sim_tasks/grab_cube.py``` \n",
    "\n",
    "We add a couple extra task-specific domain randomizations:\n",
    "- cube size\n",
    "- cube friction\n",
    "- cube color\n",
    "\n",
    "Actual values are shown below:\n",
    "\n",
    "```python\n",
    "# grab cube and return to rest keyframe\n",
    "@register_env(\"GrabCube-v1\", max_episode_steps=75)\n",
    "class GrabCubeEnv(Sim4RealBaseEnv):\n",
    "    # Task DR\n",
    "    spawn_box_half_size = 0.1 / 2\n",
    "\n",
    "    cube_size_mean = 0.017 / 2 # half size\n",
    "    cube_size_std = 7e-4\n",
    "\n",
    "    cube_friction_mean = 0.3\n",
    "    cube_friction_std = 0.05\n",
    "\n",
    "    rand_cube_color = True\n",
    "\n",
    "    noise_qpos = True\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

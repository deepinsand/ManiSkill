{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koch v1.1 Sim2Real Tutorial (BETA)\n",
    "\n",
    "This notebooks covers a general sim2real pipeline:\n",
    "1. Setting up the sim environment\n",
    "2. Setting up the real environment\n",
    "2. Aligning cameras between sim and real\n",
    "3. Domain Randomizations for training (TODO: done in code, need section explaining and allowing toggling)\n",
    "4. Training your agent with PPO\n",
    "5. Zero-Shot Evaluation on real robot (pictured below)\n",
    "\n",
    "Following the steps of this pipeline, you will replicate:\n",
    "\n",
    "MS3 custom sim training\n",
    "\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_grab_training.gif?raw=true) \n",
    "\n",
    "Zero-Shot Eval on real\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_grabcube.gif?raw=true)\n",
    "\n",
    "**Required Materials:**\n",
    "\n",
    "The comprehensive list of physical items necessary for the real environment:\n",
    "* local machine (local gpu not required)\n",
    "* Koch v1.1 robot \n",
    "* phone (or any device) camera \n",
    "* camera tripod TODO (xhin): show 3d printed tripod here\n",
    "* 3D printed 2cm sidelength cube TODO (xhin): list stl here\n",
    "* measuring tape / ruler\n",
    "\n",
    "*We also use a single lightsource of white light in our real evaluation environment\n",
    "\n",
    "**NOTE:**\n",
    "* All GPU operations in this tutorial can run on Colab's free tier, but a local machine is still required for controlling the actual robot.\n",
    "* This project currently is in a **beta release**, so not all features have been added in yet and there may be some bugs. If you find any bugs or have any feature requests please post them to our [GitHub issues](https://github.com/haosulab/ManiSkill/issues/) or discuss about them on [GitHub discussions](https://github.com/haosulab/ManiSkill/discussions/). We also have a [Discord Server](https://discord.gg/x8yUZe5AdN) through which we make announcements and discuss about ManiSkill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Setting up your Koch v1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Before We Can Start\n",
    "\n",
    "1. You first need to follow [Lerobot's tutorial for the Koch v1.1 robot setup](https://github.com/huggingface/lerobot/blob/380b836eee5f7cd61f023ef09dd5e0d9721d8c54/examples/7_get_started_with_real_robot.md). It will result in the creation of a ```Koch.yaml``` file and a calibration directory under ```.cache/calibration/koch``` required for this tutorial.\n",
    "\n",
    "2. Once finished, git clone MS3 on local: ```git clone https://github.com/haosulab/ManiSkill.git```\n",
    "3. Finally, move the ```koch.yaml``` file and a calibration directory under ```.cache/calibration/koch``` into the parent of the local, cloned root directory: ```ManiSkill/..```\n",
    "4. 3D print cube(s) using our stl: TODO xhin: add stl and path here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Compatibility for MS3\n",
    "\n",
    "Next, within the yaml, update the name of your camera to base_camera:\n",
    "\n",
    "```yaml\n",
    "[...]\n",
    "cameras:\n",
    "  base_camera:\n",
    "    _target_: lerobot.common.robot_devices.cameras.opencv.OpenCVCamera\n",
    "    camera_index: 0\n",
    "    fps: 30\n",
    "    width: 640\n",
    "    height: 480\n",
    "```\n",
    "This allows maniskill to correctly pair the real camera(s) (that you just identified for the yaml above) with the camera(s) created in the digital twin simulation environments \n",
    "\n",
    "In this tutorial, we will be using a single camera for our observations, with id=\"base_camera\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Digital Twins Simulation Evironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Understanding the Sim Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Experienced MS3 users may be able to skip to step 1.2\n",
    "\n",
    "Before we get to customizing our environments, lets first \n",
    "1. Create the default environment - used for all of the training and evaluations on our end\n",
    "2. Understand robot input & output\n",
    "3. Understand the sim robot controller\n",
    "4. Control our sim robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Making the Default Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our imports\n",
    "import mani_skill.envs\n",
    "from mani_skill.utils.wrappers.flatten import FlattenRGBDObservationWrapper\n",
    "\n",
    "# basic imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# Note: Warning aobut Greenscreening ok and will be covered in a later section\n",
    "example_env_kwargs = dict(obs_mode=\"rgb+segmentation\", render_mode=\"rgb_array\", control_mode=\"pd_joint_delta_pos\", num_envs=1, sim_backend=\"cpu\")\n",
    "sim_env = gym.make(\"GrabCube-v1\", dr=False, **example_env_kwargs)\n",
    "sim_obs, _ = sim_env.reset()\n",
    "\n",
    "print(\"simulation render:\")\n",
    "plt.imshow(sim_env.render()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Understanding Policy Input & Output\n",
    "\n",
    "Observations are either state or camera observations\n",
    "\n",
    "All state observations we use are available / estimatable during runtime in real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy input example - observing the observations\n",
    "\n",
    "# state observations\n",
    "state_obs = sim_obs[\"extra\"]\n",
    "# camera observations\n",
    "camera_obs = sim_obs[\"sensor_data\"][\"base_camera\"][\"rgb\"][0]\n",
    "\n",
    "print(\"state observations:\")\n",
    "for obs in state_obs:\n",
    "    print(obs, state_obs[obs])\n",
    "print() \n",
    "print(\"camera observations:\")\n",
    "plt.imshow(camera_obs); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A vigilant reader may notice that we use the state based boolean value of is_grasped in our observations__\n",
    "* While this is priveledged state based info in sim, all state observations in real are functions of qpos and target_qpos\n",
    "* In real, we estimate is_grasped by looking for a large difference in the target and actual qpos of the gripper joint (See Optional section at end for code implementation)\n",
    "\n",
    "Now, lets take a look at policy output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy output example - environment action space\n",
    "sample_action = sim_env.action_space.sample()\n",
    "print(\"sample_action\", sample_action)\n",
    "print(\"sample_actio shape\", sample_action.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Policy Raw Output to Robot Control Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use MS3's pd_joint_target_delta_pos controller (best performance on real robot empirically), meaning:\n",
    "* Policy (neural network) output logits are clipped between [-1,1] then scaled between ```lower``` and ```upper``` controller parameters\n",
    "* The clipped and scaled output is used for target delta control, where:\n",
    "    * qpos (length 6) are the current joint positions of the robot at each step\n",
    "    * target_qpos (length 6) are the joint positons the robot is attempting to match at each step\n",
    "    * these clipped and scaled outputs (also length 6) are added directly to target_qpos at each step\n",
    "\n",
    "The config for the controller can be found in ```ManiSkill/mani_skill/agents/robots/koch/koch.py``` and looks like:\n",
    "```python\n",
    "pd_joint_target_delta_pos = PDJointPosControllerConfig(\n",
    "    [joint.name for joint in self.robot.active_joints],\n",
    "    lower=[-0.05, -0.05, -0.05, -0.05, -0.1, -0.05],\n",
    "    upper=[0.05, 0.05, 0.05, 0.05, 0.1, 0.05],\n",
    "    stiffness=[123, 50, 102.68, 145, 108.37, 93.3],\n",
    "    damping=[15.85, 6, 15.34, 16, 16.31, 16.3],\n",
    "    force_limit=100,\n",
    "    use_delta=True,\n",
    "    use_target=True,\n",
    ")\n",
    "```\n",
    "\n",
    "We will reuse this exact controller for the real robot evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Controlling The Sim Robot\n",
    "\n",
    "rerun the following cell as many times as you'd like! (it may take many steps for noticeable difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#sim_env.reset() # uncomment this to reset the env, make it a comment to stop resetting\n",
    "\n",
    "random_action = sim_env.action_space.sample()\n",
    "close_gripper_only = np.array([0,0,0,0,0,-1.0])\n",
    "action = close_gripper_only # try random_action also!\n",
    "sim_obs, _, _, _, _ = sim_env.step(action)\n",
    "\n",
    "plt.imshow(sim_obs[\"sensor_data\"][\"base_camera\"][\"rgb\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Choosing Simulation Camera Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make your own digital twin... a twin, this pipeline estimates sim camera pose, by:\n",
    "\n",
    "* Setting necessary LookAt transform parameters in sim: \n",
    "    * camera position\n",
    "    * camera target position\n",
    "    * camera fov\n",
    "* Choosing 3D alignment point(s) for camera alignment:\n",
    "    * set the alignment dot(s) so they span your simulation camera's field of view (i.e. they are not all in one place)\n",
    "    * measuring them out carefully in real (section 2)\n",
    "* Aligning real camera pose(s) with sim, using our script (section 3)\n",
    "\n",
    "The following code cell sets the **_LookAt transform and camera alignment parameters_** for the simulated environment\n",
    "\n",
    "* **We recommend leaving the default values for now, coming back to change them as necessary once you set up your real environment**\n",
    "\n",
    "* **Differing camera positions are currently untested and may make perception more difficult**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Simulation: Coordinates are in meters, and they are centered at the robot's base (at the edge of the table, pictured below):\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_base_pos.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### User parameters for LookAt transform####\n",
    "# NOTE: all values are offsets from robot base position (see following cell)\n",
    "robot_base_color = [0.95,0.95,0.95]\n",
    "\n",
    "base_camera_pos = [0.40, 0 + 0.265, 0.1725]\n",
    "camera_target = [0.2, 0, 0] # green dot in sim debug mode\n",
    "\n",
    "# tune this according to your camera\n",
    "# TODO (xhin): should honestly be a whole different section / reference a tutorial for camera intrinsics estimation\n",
    "camera_fov = 52* (np.pi / 180)\n",
    "\n",
    "# debug camera position points - appear if debug in rgb_overlay_mode, useful in camera alignment\n",
    "# really only need 2, but more is helpful\n",
    "alignment_dots = [ # yellow dots in sim debug mode\n",
    "    [0.2,  0.10, 0], ## close to camera\n",
    "    [0.2,  -0.1, 0], ## far from camera\n",
    "    [0.35, 0.00, 0], ## far infront of robot\n",
    "    [0.35, 0.10, 0], ## far infront of robot and close camera\n",
    "]\n",
    "#### End User parameters ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render with Camera Parameters\n",
    "dubug = True # toggles visibility of alignment dots\n",
    "# all user params from cell above applied via env kwargs\n",
    "example_env_kwargs.update(robot_base_color=robot_base_color,\n",
    "                          base_camera_pos=base_camera_pos,\n",
    "                          camera_target=camera_target,\n",
    "                          camera_fov=camera_fov,\n",
    "                          alignment_dots=alignment_dots)\n",
    "sim_env = gym.make(\"GrabCube-v1\", dr=False, debug=dubug, **example_env_kwargs)\n",
    "sim_obs, _ = sim_env.reset(seed=1)\n",
    "\n",
    "\n",
    "sim_env.unwrapped.toggle_greenscreen(False) # covered in section 1.3\n",
    "env_render = sim_env.render().cpu()\n",
    "print()\n",
    "print(\"Visualization of camera alignment dots: Alignment dots appear if env 'debug' mode is on\")\n",
    "print(\"The Green dot marks camera lookat target (center of rendered image)\")\n",
    "print(\"Yellow dots mark camera alignment dots\")\n",
    "plt.imshow(env_render[0]); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 Saving Custom Env Params to Disk\n",
    "* The cell below stores the custom parameters you just set in a json file to use later for camera alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_env.unwrapped.save_user_kwargs(\"user_kwargs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 GreenScreening in ManiSkill3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can overlay our robot and scene actors on any image we would like, for example: a green-screen\n",
    "* MS3 stores segmentation data available during rasterization, at each rendering step\n",
    "* Later, we use the real environment background (from step 3) for simulation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a \"green-screen\"\n",
    "dummy_greenscreen = np.zeros((128,128,3))\n",
    "dummy_greenscreen[:,:,1] = 0.75\n",
    "plt.imsave(\"dummy_greenscreen.png\", dummy_greenscreen)\n",
    "\n",
    "# temporarily turn off domain randomiations\n",
    "domain_rands = False\n",
    "\n",
    "# now we can create the sim environment, and view it\n",
    "# obs_mode=\"rgb+segmentation\" required for greenscreening \n",
    "# reusing user params saved within example_env_kwargs \n",
    "sim_env = gym.make(\"GrabCube-v1\", rgb_overlay_path=\"dummy_greenscreen.png\", dr=domain_rands, **example_env_kwargs)\n",
    "sim_obs, _ = sim_env.reset(seed=0)\n",
    "\n",
    "# view the simulation renders\n",
    "\n",
    "# toggle off greenscreening for render - note that greenscreening for sim_env.get_obs() stays on!\n",
    "# greenscreening activated by default\n",
    "sim_env.unwrapped.toggle_greenscreen()\n",
    "print(\"Green-Screening off\")\n",
    "plt.imshow(sim_env.render().cpu()[0]); plt.show()\n",
    "\n",
    "# toggle back on greenscreeening\n",
    "sim_env.unwrapped.toggle_greenscreen()\n",
    "print(\"Green-Screening On (render resolution adjusted to env observation res 128x128)\")\n",
    "plt.imshow(sim_env.render().cpu()[0]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the greenscreen can be anything we'd like\n",
    "!wget -O other_greenscreen.png https://www.dinehq.com/uploads/maniskill.png > /dev/null 2>&1\n",
    "sim_env.unwrapped.set_overlay(camera_name=\"base_camera\", path=\"other_greenscreen.png\")\n",
    "print(\"Green-Screen Changed (render resolution adjusted to env observation res 128x128)\")\n",
    "plt.imshow(sim_env.render().cpu()[0]); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Setting Up your Real Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Drawing the Alignment Dots\n",
    "* Draw the alignment dots from section 1.2 on your real table (more accurate) or place your choice of removable markers on the 3D alignment points we passed into the environment earlier\n",
    "    *  Color/shape/size doesn't matter here, this is only for aligning the camera\n",
    "* Similarly, draw/place a dot for the camera target position\n",
    "* Finally, measure and place your camera at chosen 3D point, and roughly align it to scene (camera alignment in section 3)\n",
    "\n",
    "**Reminder** Coordinates are in meters, and they are centered at the robot's base (at the edge of the table, pictured below):\n",
    "* Measure out the alignment dots using the values set (default or your chosen values) in 1.2, centered at point visualized below\n",
    "* If the default values are incompatible with your setup requirements, this is where you would change the camera setup parameters in 1.2\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_base_pos.png?raw=true)\n",
    "\n",
    "**Note:** you do not need to perfectly pinpoint this robot base position\n",
    "* An estimate point on the edge of the table will suffice; the robot can be further adjusted during camera alignment in section 3. \n",
    "\n",
    "After this step, our setup looks like this:\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_real_init.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Matching Real Camera Pose with Sim Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script overlays the simulation actors onto your real world setup\n",
    "\n",
    "**NOTE**: Before running any scripts with the real robot, we suggest you ensure that your ports are correctly listed in your Koch.yaml file, using the ```find_motors_bus_port.py``` script from lerobot and re-permit use of the ports on local with chmod if necessary, and then plug in your robots\n",
    "\n",
    "1 Download and paste ```user_kwargs.json``` from this colab into the local ```ManiSkill/..``` directory, for custom env replication on local\n",
    "\n",
    "2 On local and within the parent of the cloned root directory: ```ManiSkill/..```\n",
    "* Ensure your ports in Koch.yaml are correctly listed \n",
    "* Plug in both leader and follower arm (we leverage all of lerobot's hardware communication code, so you must plug in the leader to work with their code)\n",
    "* Run: ```python3 ManiSkill/mani_skill/envs/tasks/digital_twins/utils/camera_setup.py --robot_yaml_path=koch.yaml --keyframe_id=elevated_turn --user_kwargs_path=user_kwargs.json```\n",
    "    * You will be prompted to press 'Enter' before the robot moves to the initial pose given by the --keyframe_id argument\n",
    "\n",
    "\n",
    "Our Aligment process looks like:\n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_cam_align.gif?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Taking the Greenscreen Photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Before running any scripts with the real robot, we suggest you ensure that your ports are correctly listed in your Koch.yaml file, using the ```find_motors_bus_port.py``` script from lerobot and re-permit use of the ports on local with chmod if necessary, then plug in your robots\n",
    "\n",
    "Once you have aligned your camera, you can take the background photo for greenscreened simulation training:\n",
    "* Move your koch_v1.1 follower arm out of camara view\n",
    "* Run ```python3 ManiSkill/mani_skill/envs/tasks/digital_twins/utils/camera_setup.py --robot_yaml_path=koch.yaml --keyframe_id=elevated_turn --output_photo_path=<your_path_here>.png```\n",
    "\n",
    "This will output a photo at ```<your_path_here>_base_camera.png```\n",
    "\n",
    "If you're in Colab:\n",
    "* Copy and paste this photo into the colab for simulation training (step 4)\n",
    "\n",
    "Feel free to re-run camera alignment from previous step to move koch_v1.1 back into its proper place, without moving the camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final background image looks like (yours may be 128x128 instead): \n",
    "\n",
    "![](https://github.com/haosulab/ManiSkill/blob/koch-v1.1/docs/source/user_guide/tutorials/images/koch_example_greenscreen.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, visualize your final greenscreen in sim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab_greenscreen_path = ... # put your path here\n",
    "example_env_kwargs.update(rgb_overlay_path=colab_greenscreen_path)\n",
    "sim_env = gym.make(\"GrabCube-v1\", dr=False, **example_env_kwargs)\n",
    "sim_obs, _ = sim_env.reset(seed=0)\n",
    "plt.imshow(sim_obs[\"sensor_data\"][\"base_camera\"][\"rgb\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-save the user_kwargs for the environment, this saves your greenscreen path as well - to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_env.unwrapped.save_user_kwargs(\"user_kwargs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Training your Agent in Sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MS3 provdes an array of rl training scripts within ```ManiSkill/examples/baselines```\n",
    "\n",
    "Below, we provide an example using the modules from MS3's ```ppo_rgb.py``` to train an agent with the environment you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/haosulab/ManiSkill/koch-v1.1/examples/baselines/ppo/ppo_rgb.py -O ppo_rgb_colab.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started first download the RL code as done below and import some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below opens tensorboard on colab so you can watch training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the script, change the colab to a GPU environment (Runtime -> Change Runtime Type)\n",
    "\n",
    "Evaluation rollout videos and model checkpoints will populate in the runs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ppo_rgb.py --env_id=\"GrabCube-v1\" --num_envs=256 \\\n",
    "    --update_epochs=8 --num_minibatches=8 --total_timesteps=15_000_000 \\\n",
    "    --num-steps=75 --num_eval_steps=75 --gamma=0.90 \\\n",
    "    --no_partial_reset --render_mode=rgb_array --reconfiguration_freq=10 \\\n",
    "    --obs_mode=rgb+segmentation --seed=0 --no_finite_horizon_gae \\\n",
    "    --exp_name=grab_cube_training --user_kwargs_path=user_kwargs.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Policy Evaluation on Real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Safe Evaluation\n",
    "\n",
    "**NOTE**: Before running any scripts with the real robot, we suggest you ensure that your ports are correctly listed in your Koch.yaml file, using the ```find_motors_bus_port.py``` script from lerobot and re-permit use of the ports on local with chmod if necessary, then plug in your robots\n",
    "\n",
    "You can evaluate any checkpoint from training directly on the real robot:\n",
    "* Copy and paste the checkpoint, e.g. ckpt_476.pt, into the parent of your local root directory: ```ManiSkill/..```\n",
    "* Run the following script on local - the debug flag requires you to press enter at every environment step\n",
    "```\n",
    "python3 ManiSkill/mani_skill/envs/tasks/digital_twins/utils/real_ppo_agent_eval.py --real_env_id=\"RealGrabCube-v1\" --robot_yaml_path=\"koch.yaml\" --keyframe_id=\"elevated_turn\" --control_mode=\"pd_joint_delta_pos\" --num_eval_steps=200 --debug --checkpoint=ckpt_476.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.2 Normal Evaluation\n",
    "\n",
    "**Warning**: Be ready to unplug / stop the script at any time; it is the user's responsibility to ensure the robot does not hurt its own joints via colliding with the table, etc. There is no guarentee the policy will properly transfer to real. We recommend thourough evaulation with --debug flag before moving onto this step.\n",
    "\n",
    "The simulation training and recommended evaluation frequence is 15hz. To evaluate at 15hz, run:\n",
    "\n",
    "```python3 ManiSkill/mani_skill/envs/tasks/digital_twins/utils/real_ppo_agent_eval.py --real_env_id=\"RealGrabCube-v1\" --robot_yaml_path=\"koch.yaml\" --keyframe_id=\"elevated_turn\" --control_mode=\"pd_joint_delta_pos\" --num_eval_steps=200 --checkpoint=ckpt_476.pt --control_freq=15```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO (xhin): add link/gif of our evaluation(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO (xhin): add optional BaseRealEnv and BaseRealAgent section\n",
    "\n",
    "- just pasting code and explaining|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO (xhin): add DR section\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
